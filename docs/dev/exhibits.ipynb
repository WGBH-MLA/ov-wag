{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df76e94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ as env\n",
    "\n",
    "env['DJANGO_SETTINGS_MODULE'] = 'ov_wag.settings.dev'\n",
    "env[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\"\n",
    "print(env['OV_DB_NAME'])\n",
    "import django\n",
    "\n",
    "django.setup()\n",
    "from wagtail.models import Page\n",
    "\n",
    "ov = Page.objects.get(id=3)\n",
    "aapb = Page.objects.get(id=59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153e8164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/home/harpo/gbh/aapb/AAPB2/exhibits.json') as f:\n",
    "    exhibits = json.load(f)\n",
    "len(exhibits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715a3beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmless.models import Exhibit\n",
    "\n",
    "exhibits = [Exhibit(**exhibit) for exhibit in exhibits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050706d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = 0\n",
    "for exhibit in exhibits:\n",
    "    # print(exhibit.title, len(exhibit.children))\n",
    "    pages += len(exhibit.children)\n",
    "    # for n, p in enumerate(exhibit.children):\n",
    "    # print(f'  {n+1}: {p.title} - {p.page}')\n",
    "\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94ff421",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wagtail.images.models import Image\n",
    "\n",
    "\n",
    "# TODO: standardize image download function across migrate scripts\n",
    "def download_image(\n",
    "    url: str, title: str | None = None, alt: str | None = None\n",
    ") -> Image | None:\n",
    "    import requests\n",
    "    from django.core.files.base import ContentFile\n",
    "    from wagtail.images import get_image_model\n",
    "\n",
    "    ImageModel = get_image_model()\n",
    "    response = requests.get(url)\n",
    "    if not title:\n",
    "        title = url.split(\"/\")[-1]\n",
    "    if response.status_code == 200:\n",
    "        image = Image(\n",
    "            file=ContentFile(response.content, name=title),\n",
    "            title=title,\n",
    "            description=alt[:255] if alt else '',\n",
    "        )\n",
    "        image.save()\n",
    "        return image\n",
    "    else:\n",
    "        print(f\"Failed to download image from {url}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113cbdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmless.parse import (\n",
    "    parse_cmless_thumbnail,\n",
    "    parse_records_markdown,\n",
    "    pasre_authors_markdown,\n",
    "    markdownify,\n",
    ")\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4850bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subheadings(md: str) -> list[tuple[str, str]]:\n",
    "    # First split by main headings (###)\n",
    "    main_heading_pattern = r'###\\s+(.+?)\\s*\\n'\n",
    "    main_parts = split(main_heading_pattern, md)\n",
    "\n",
    "    if len(main_parts) == 1:\n",
    "        return [('text', markdownify(md))]\n",
    "\n",
    "    sections = []\n",
    "    # Skip the first element (content before any heading) and pair up headings with content\n",
    "    for i in range(1, len(main_parts), 2):\n",
    "        if i < len(main_parts) - 1:\n",
    "            heading = main_parts[i].strip()\n",
    "            content = main_parts[i + 1].strip()\n",
    "\n",
    "            if not content:\n",
    "                continue\n",
    "\n",
    "            # Add the main heading\n",
    "            sections.append(('heading', markdownify(heading)))\n",
    "\n",
    "            # Now check if this content has subheadings (####)\n",
    "            subheading_pattern = r'####\\s+([^\\n]+)'\n",
    "            sub_parts = split(subheading_pattern, content)\n",
    "\n",
    "            # If there are subheadings, parse them\n",
    "            if len(sub_parts) > 1:\n",
    "                # First part is text before any subheading\n",
    "                main_text = sub_parts[0].strip()\n",
    "                if main_text:\n",
    "                    sections.append(('text', markdownify(main_text)))\n",
    "\n",
    "                # Parse subheading pairs\n",
    "                for j in range(1, len(sub_parts), 2):\n",
    "                    sub_heading = sub_parts[j].strip()\n",
    "                    sections.append(('subheading', markdownify(sub_heading)))\n",
    "                    # Check if there's content after this subheading\n",
    "                    if j + 1 < len(sub_parts):\n",
    "                        sub_text = sub_parts[j + 1].strip()\n",
    "                        if sub_text:\n",
    "                            sections.append(('text', markdownify(sub_text)))\n",
    "            else:\n",
    "                # No subheadings, just add the content as text\n",
    "                sections.append(('text', markdownify(content)))\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89657195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aapb_exhibits.models import AAPBExhibit, AAPBExhibitsChildOrder\n",
    "from authors.models import Author, AAPBAuthorsOrderable\n",
    "from re import split\n",
    "\n",
    "\n",
    "def create_exhibit_page(exhibit: Exhibit) -> AAPBExhibit:\n",
    "\n",
    "    body = []\n",
    "    if exhibit.main:\n",
    "        body += extract_subheadings(exhibit.main)\n",
    "    if exhibit.extended:\n",
    "        body += extract_subheadings(exhibit.extended)\n",
    "    if exhibit.resources:\n",
    "        body.append(('heading', 'Resources'))\n",
    "        body.append(('text', markdownify(exhibit.resources)))\n",
    "    if exhibit.records:\n",
    "        records = parse_records_markdown(exhibit.records)\n",
    "        if records:\n",
    "            # body.append(('heading', 'Records'))\n",
    "            body.append(('records', {'guids': '\\n'.join(records)}))\n",
    "\n",
    "    if exhibit.title.find('<em>') >= 0 or exhibit.title.find('*') >= 0:\n",
    "        display_title = markdownify(exhibit.title)\n",
    "    title = exhibit.title.replace('<em>', '').replace('</em>', '').replace('*', '')\n",
    "\n",
    "    if exhibit.cover:\n",
    "        cover = BeautifulSoup(exhibit.cover)\n",
    "        if cover.img:\n",
    "            cover_image = download_image(\n",
    "                url=cover.img.get('src'),\n",
    "                title=cover.img.get('title', exhibit.title),\n",
    "                alt=cover.img.get('alt'),\n",
    "            )\n",
    "    page = AAPBExhibit(\n",
    "        title=title,\n",
    "        display_title=(display_title if 'display_title' in locals() else None),\n",
    "        slug=exhibit.slug,\n",
    "        introduction=markdownify(exhibit.summary) if exhibit.summary else '',\n",
    "        body=body,\n",
    "        cover_image=(cover_image if 'cover_image' in locals() else None),\n",
    "        # gallery = exhibit.gallery,\n",
    "    )\n",
    "    if exhibit.authors:\n",
    "        authors_markdown = pasre_authors_markdown(exhibit.authors)\n",
    "        authors = []\n",
    "        for author in authors_markdown:\n",
    "            author_obj, created = Author.objects.get_or_create(name=author.name)\n",
    "            if created:\n",
    "                author_obj.title = author.title\n",
    "                if author.image:\n",
    "                    author_obj.image = download_image(\n",
    "                        url=author.image,\n",
    "                        title=author.name,\n",
    "                        alt=author.name,\n",
    "                    )\n",
    "                # author_obj.bio =\n",
    "                author_obj.save()\n",
    "            author_ord = AAPBAuthorsOrderable(\n",
    "                author=author_obj,\n",
    "                page=page,\n",
    "            )\n",
    "            authors.append(author_ord)\n",
    "\n",
    "        page.authors.set(authors)\n",
    "    return page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156b9249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Show the actual content being split\n",
    "for exhibit in exhibits:\n",
    "    if '####' in exhibit.main:\n",
    "        print(f\"Exhibit: {exhibit.title}\\n\")\n",
    "\n",
    "        from re import split\n",
    "\n",
    "        main_heading_pattern = r'###\\s+(.+?)\\s*\\n'\n",
    "        main_parts = split(main_heading_pattern, exhibit.main)\n",
    "\n",
    "        # Check the Acknowledgements section specifically\n",
    "        for i in range(1, len(main_parts), 2):\n",
    "            if i < len(main_parts) - 1:\n",
    "                heading = main_parts[i].strip()\n",
    "                if 'Acknowledge' in heading:\n",
    "                    content = main_parts[i + 1]\n",
    "                    print(f\"Section: {heading}\")\n",
    "                    print(f\"Content (last 300 chars):\\n{content[-300:]}\")\n",
    "                    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "                    # Try splitting\n",
    "                    subheading_pattern = r'####\\s+([^\\n]+)\\n'\n",
    "                    sub_parts = split(subheading_pattern, content)\n",
    "                    print(f\"\\nSplit into {len(sub_parts)} parts:\")\n",
    "                    for j, part in enumerate(sub_parts):\n",
    "                        print(f\"\\n[{j}] ({'HEADING' if j % 2 == 1 else 'TEXT'}):\")\n",
    "                        print(repr(part[:150]) if part else \"(empty)\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c80474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for exhibit in exhibits:\n",
    "    print(f'Creating exhibit: {exhibit.title}')\n",
    "    page = create_exhibit_page(exhibit)\n",
    "    aapb.add_child(instance=page)\n",
    "    page.save_revision().publish()\n",
    "    child_pages = []\n",
    "    for child in exhibit.children:\n",
    "        print(f'  Creating child page {child.page}: {child.title}')\n",
    "        child_page = create_exhibit_page(child)\n",
    "        page.add_child(instance=child_page)\n",
    "        child_page.save_revision().publish()\n",
    "        # Add child order entry\n",
    "        child_order_entry = AAPBExhibitsChildOrder(\n",
    "            exhibit=child_page,\n",
    "            page=page,\n",
    "        )\n",
    "        child_pages.append(child_order_entry)\n",
    "    page.child_order.set(child_pages)\n",
    "    page.save_revision().publish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
